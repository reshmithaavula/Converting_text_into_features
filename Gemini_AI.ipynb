{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNc9zWiB2wCeyoVUxUb1+rp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reshmithaavula/Converting_text_into_features/blob/main/Gemini_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nV6mSbhZcijj"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q \"google-generativeai>=0.7.2\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "Ll0ihWkidaAO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=genai.GenerativeModel('models/gemini-2.0-flash')\n",
        "response=model.generate_content(\"please give me python code to sort a list.\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JU2z70y2gyVQ",
        "outputId": "abfc6c3b-eeec-4854-f62c-0221197f541a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are a few ways to sort a list in Python, along with explanations of when to use each:\n",
            "\n",
            "**1. `list.sort()` (In-place sorting):**\n",
            "\n",
            "   *   Modifies the original list directly.\n",
            "   *   Returns `None`.\n",
            "   *   Generally faster than `sorted()` when you don't need to keep the original list.\n",
            "\n",
            "   ```python\n",
            "   my_list = [3, 1, 4, 1, 5, 9, 2, 6]\n",
            "   my_list.sort()  # Sorts the list in ascending order\n",
            "   print(my_list)  # Output: [1, 1, 2, 3, 4, 5, 6, 9]\n",
            "\n",
            "   my_list.sort(reverse=True)  # Sorts in descending order\n",
            "   print(my_list)  # Output: [9, 6, 5, 4, 3, 2, 1, 1]\n",
            "   ```\n",
            "\n",
            "**2. `sorted()` (Creates a new sorted list):**\n",
            "\n",
            "   *   Creates a *new* sorted list from the iterable (can be a list, tuple, string, etc.).\n",
            "   *   Leaves the original iterable unchanged.\n",
            "   *   Useful when you want to preserve the original data.\n",
            "\n",
            "   ```python\n",
            "   my_list = [3, 1, 4, 1, 5, 9, 2, 6]\n",
            "   sorted_list = sorted(my_list)  # Ascending order\n",
            "   print(my_list)      # Output: [3, 1, 4, 1, 5, 9, 2, 6] (original list unchanged)\n",
            "   print(sorted_list)  # Output: [1, 1, 2, 3, 4, 5, 6, 9]\n",
            "\n",
            "   reversed_list = sorted(my_list, reverse=True)  # Descending order\n",
            "   print(reversed_list) # Output: [9, 6, 5, 4, 3, 2, 1, 1]\n",
            "   ```\n",
            "\n",
            "**3. Custom Sorting with `key`:**\n",
            "\n",
            "   * Both `list.sort()` and `sorted()` accept a `key` argument.\n",
            "   * `key` is a function that takes an element from the list and returns a value to be used for comparison during the sorting process.\n",
            "   * Extremely powerful for sorting based on complex criteria.\n",
            "\n",
            "   ```python\n",
            "   # Sort a list of strings by their length\n",
            "   strings = [\"apple\", \"banana\", \"kiwi\", \"orange\"]\n",
            "   strings.sort(key=len)\n",
            "   print(strings)  # Output: ['kiwi', 'apple', 'banana', 'orange']\n",
            "\n",
            "   # Sort a list of tuples by the second element\n",
            "   tuples = [(1, 'z'), (2, 'a'), (3, 'b')]\n",
            "   sorted_tuples = sorted(tuples, key=lambda x: x[1])  # Using a lambda function\n",
            "   print(sorted_tuples)  # Output: [(2, 'a'), (3, 'b'), (1, 'z')]\n",
            "\n",
            "   #Sort a list of dictionaries by a specific key in the dictionary\n",
            "   people = [\n",
            "    {'name': 'Alice', 'age': 30},\n",
            "    {'name': 'Bob', 'age': 25},\n",
            "    {'name': 'Charlie', 'age': 35}\n",
            "   ]\n",
            "   sorted_people = sorted(people, key=lambda person: person['age'])\n",
            "   print(sorted_people)\n",
            "   # Output:\n",
            "   # [{'name': 'Bob', 'age': 25}, {'name': 'Alice', 'age': 30}, {'name': 'Charlie', 'age': 35}]\n",
            "   ```\n",
            "\n",
            "**Choosing the Right Method:**\n",
            "\n",
            "*   **`list.sort()`:**  Use when you want to modify the original list in place and don't need to keep a copy. This is usually the fastest option.\n",
            "\n",
            "*   **`sorted()`:**  Use when you want to create a new sorted list and preserve the original list. This is more memory-intensive because it creates a copy. Also useful if you're sorting an iterable that's not a list (e.g., a tuple, string, dictionary keys, etc.).\n",
            "\n",
            "*   **`key` argument:** Use in both `list.sort()` and `sorted()` when you need to sort based on a complex criteria or attribute of the elements in the list.  Lambda functions are often used to define the `key` function concisely.\n",
            "\n",
            "**Example for Numbers and Strings:**\n",
            "\n",
            "```python\n",
            "numbers = [5, 2, 8, 1, 9, 4]\n",
            "strings = [\"banana\", \"apple\", \"orange\", \"grape\"]\n",
            "\n",
            "# Sort numbers in ascending order\n",
            "numbers.sort()\n",
            "print(numbers)  # Output: [1, 2, 4, 5, 8, 9]\n",
            "\n",
            "# Sort strings alphabetically\n",
            "strings.sort()\n",
            "print(strings)  # Output: ['apple', 'banana', 'grape', 'orange']\n",
            "```\n",
            "\n",
            "**Important Notes:**\n",
            "\n",
            "*   Python's sorting algorithms are generally very efficient (Timsort, a hybrid merge sort and insertion sort).\n",
            "*   The `key` argument can have a significant impact on performance if the key function is computationally expensive.\n",
            "*   For very large lists, consider using specialized sorting algorithms or libraries like NumPy if you need maximum performance and have numerical data.\n",
            "\n",
            "Remember to choose the sorting method that best suits your specific needs and desired outcome.  The `key` argument provides the greatest flexibility for custom sorting scenarios.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "client=genai.Client(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "2Vkizn5UhbJc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=model.generate_content(\"What is the large language model\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "eTHcwvjQjsjM",
        "outputId": "3dfda513-e1e4-4e96-b32c-4cd32ae01dc1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A large language model (LLM) is a type of artificial intelligence (AI) model designed to understand and generate human language. It's \"large\" because it's trained on a massive amount of text data, often billions or even trillions of words, which allows it to learn complex patterns and relationships within the language.\n",
            "\n",
            "Here's a breakdown of key aspects of LLMs:\n",
            "\n",
            "**Key Characteristics:**\n",
            "\n",
            "*   **Trained on Massive Datasets:**  LLMs are fed enormous amounts of text data, often scraped from the internet (websites, books, articles, code, etc.). This huge dataset enables them to learn the statistical probabilities of word sequences and grammatical structures.\n",
            "*   **Neural Networks:**  They are based on neural networks, a type of machine learning model inspired by the structure of the human brain.  Specifically, the most successful LLMs are built on the **Transformer architecture**, which is particularly well-suited for processing sequential data like text.\n",
            "*   **Self-Supervised Learning:**  A crucial element is self-supervised learning. LLMs are typically trained by predicting the next word in a sequence, or masking words and trying to predict them.  This allows them to learn without explicit human labeling of the data. They learn by example, figuring out the relationships between words and phrases from the raw text itself.\n",
            "*   **Generative Capabilities:**  LLMs can generate new text, translate languages, write different kinds of creative content, and answer your questions in an informative way.  They are *generative* in that they can create new outputs, rather than simply classifying or analyzing existing data.\n",
            "*   **Contextual Understanding:**  They can understand the context of a conversation or a piece of text and generate responses that are relevant and coherent.  The Transformer architecture allows them to consider long-range dependencies between words, which is crucial for understanding context.\n",
            "*   **Few-Shot or Zero-Shot Learning:**  Some advanced LLMs can perform tasks with very few (few-shot) or even no (zero-shot) examples.  This means they can adapt to new tasks without requiring extensive fine-tuning.\n",
            "\n",
            "**How They Work (Simplified):**\n",
            "\n",
            "1.  **Training:** The model is fed vast amounts of text data.\n",
            "2.  **Pattern Recognition:**  During training, the model learns the statistical relationships between words, phrases, and grammatical structures.  It builds a massive internal representation of language.\n",
            "3.  **Prediction:**  When you give the model a prompt (e.g., a question or a starting sentence), it uses its learned knowledge to predict the most likely next word (or sequence of words) in the response.  It continues generating text word by word until it reaches a stopping point.\n",
            "\n",
            "**Examples of LLMs:**\n",
            "\n",
            "*   **GPT Series (e.g., GPT-3, GPT-4):** Developed by OpenAI, known for their strong text generation capabilities.\n",
            "*   **LaMDA:**  Developed by Google, designed for conversational applications.\n",
            "*   **BERT:**  Developed by Google, excels at understanding the context of words in a sentence.  Often used for tasks like sentiment analysis and question answering.\n",
            "*   **LLaMA (Large Language Model Meta AI):** A family of open-source LLMs released by Meta.\n",
            "*   **PaLM (Pathways Language Model):**  Another LLM from Google.\n",
            "\n",
            "**Applications:**\n",
            "\n",
            "*   **Chatbots and Virtual Assistants:**  Powering conversational AI systems.\n",
            "*   **Content Creation:**  Generating articles, blog posts, marketing copy, and creative writing.\n",
            "*   **Translation:**  Automatically translating text between languages.\n",
            "*   **Summarization:**  Condensing long documents into shorter summaries.\n",
            "*   **Code Generation:**  Writing code based on natural language descriptions.\n",
            "*   **Question Answering:**  Providing answers to questions based on a knowledge base.\n",
            "*   **Search Engines:**  Improving search results by understanding the meaning behind queries.\n",
            "*   **Customer Service:**  Automating customer support interactions.\n",
            "*   **Sentiment Analysis:**  Determining the emotional tone of text.\n",
            "\n",
            "**Limitations:**\n",
            "\n",
            "*   **Bias:**  LLMs can reflect biases present in the training data, leading to unfair or discriminatory outputs.\n",
            "*   **Lack of Real-World Understanding:**  They primarily understand language patterns and may not have a deep understanding of the real world.\n",
            "*   **Hallucination:**  They can sometimes generate incorrect or nonsensical information (often referred to as \"hallucinations\").\n",
            "*   **Computational Cost:**  Training and running LLMs can be very expensive in terms of computing resources.\n",
            "*   **Ethical Concerns:**  Potential misuse for spreading misinformation, creating deepfakes, and automating tasks that could displace human workers.\n",
            "\n",
            "**In summary, a large language model is a powerful AI model trained on massive amounts of text data, capable of understanding and generating human language with remarkable fluency and coherence.  They are transforming many industries and applications, but it's important to be aware of their limitations and ethical implications.**\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID=\"gemini-2.0-flash\""
      ],
      "metadata": {
        "id": "hW8R7YVRlcU2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "response=client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the largest planet in our solar system?\"\n",
        ")\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "WnhW-pEllrsa",
        "outputId": "9e4f1d35-109f-4c3e-e953-5df0146c1c81"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The largest planet in our solar system is **Jupiter**.\n"
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the higheset mountian in the world?\"\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_zUiLUJmSsf",
        "outputId": "5474168f-0226-4378-f74e-26022f86edb4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_tokens=13 cached_content_token_count=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NcKzcvagniKw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}